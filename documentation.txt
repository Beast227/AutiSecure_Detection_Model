--------------Finished work---------------------

1. Data collection and preprocessing
2. File structure
3. Predict and save the model
4. Finished detection of aggression and object lining up
2. Start preparing to get the traits from the video

3. Prepare the data for the traits
1. Test the model
4. Train the model for the traits
5. Test the model for the traits


--------------To do list-------------------
1

----------------Working-------------------
 -> Detection of eye contact or gaze
    - It detects the eye centering with the help of face center horizontally and vertically.
    - Then it checks weather the gaze ratio is above the threshold or not.
    - It return 1 if the gaze ratio exceeds the threshold otherwise 0.
    - OpenCV is used to read the video files and google's mediapipe is used to detect the gaze.
    - The gaze ratio is calculated by dividing the horizontal distance between the face center and the eye center
      by the horizontal distance between the face center and the left eye center.

 -> Detection of aggressive behavior
    - It detects the aggressive behavior with the help of these characteristics
        1. Fast motion like push or punch
        2. Aggressive emotion
    - Fast motion is calculated with the help of pythagoras theorem which helps in calculating the distance between old positon
      and the new postion of the face.
    - Aggressive emotion is detected with the help of FER pretrained model.
    - The only drawback of this algorithm is it takes good amount of time to process the video and give the results

-> Detect hyper or hypo reactivity
    - Hyper reactivity is sudden spikes in motion
    - Hypo reactivity is long streches of very low motion
    - It reads the video with the help of openCV, skips frames to improve performance
    - Calculates motion scores if its below or above the threshold for hypo and hyper reactivity   
    - Based on the continuous streaks for the hypo or hyper reactivity the output will be 0 or 1 (checks frames after some frame skip)
    - Limitations: 
        - Lighting conditions can affect the output
        - If any pet is visible in the video that can also affect the video

-> Detect non responsiveness 
    - The concept of non-responsiveness detection in videos is about identifying prolonged periods where a person shows little to no movement and no visible signs of engagement (like face detection). This is done by analyzing both motion (using optical flow to see if the body is moving) and face engagement (using a face detector). If the video shows very low movement (stillness) and no face is detected for a sustained duration (e.g., 8 seconds), it is flagged as non-responsiveness. This approach is useful in areas like health monitoring, therapy, and safety systems, where detecting if someone has become unresponsive is important.

-> Detect no typical language
    - This detects two types of a way to speak.
        1. Echolalia - Repetition of same words
        2. Monotony - Lack of tone while speaking
    - First it extracts the audio from the video and saves it as WAV file
    - Splits the audio into speech segments with the help of silences in between.
    - Converts each audio speech into numeric fingerprint 
    - Compares fingerprint with the cosine of similarity to detect similarity in words
    - checks variablility in speech durations and silence gaps
    - cleans up temp and return a dictionary { 'echolalia': 0/1, 'monotony': 0/1 }

-> Detect object lining up
    - Reads the video and gets the total frame count and fps from cv2
    - Detects objects from the video with the help of pretrained models
    - finds the center of the objects
    - checks if it can form a straight line
    - checks if they are evenly spaced
    - if the pattern happens accross frequent frames then it says lining up detected

-> Detect self spinning
    - Reads the video frame by frame
    - Detects body landmarks using mediapipe
    - compute torso angle (rotation of the body)
    - compute torso angle across frames
    - if many frames in a row shows large rotations then classify it as self spinning
    - return true if spinning is detected

-> Detect self hitting
    - Reads the video 
    - Detects the position of wrists and nose
    - Measures distance between hands and head
    - if hand is very close to the head marks as a possible hit
    - Needs 10 consecutive hits for confirming
    - Returns 1 if self hitting is detected

-> Detect upper limb stereotypes
    - Reads the video
    - Track wrists relative to the shoulders/head
    - for each frames it checks if the wrists are moving fast or are they close to the head/body or repetitive motion back and forth
    - if many frames satisfy all three then uppen stereotype limb detected